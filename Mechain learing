##### 决策树

1.分类决策树模型是表示基于特征对实例进行分类的树形结构。决策树可以转换成一个**if-then**规则的集合，也可以看作是定义在特征空间划分上的类的条件概率分布。

2.决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的“分而治之”(divide-and-conquer)策略。

 决策树学习算法包括3部分：特征选择、树的生成和树的剪枝。常用的算法有ID3、 C4.5和CART。

3.特征选择的目的在于选取对训练数据能够分类的特征。特征选择的关键是最优划分属性。

“信息嫡”(information entropy)是度量样本集合纯度最常用的一种指标。假定当前样本集合D中第k类样本所占的比例为$p_k(k = 1,2,...,|\mathcal{Y}|)，$,则D的信息嫡定义为
$$
\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|}p_k\log_2p_k
$$
$Ent(D)$的值越小，则D的纯度越高。

常用的准则如下：

（1）样本集合V对特征a的信息增益$ID3$​
$$
Gain(D,a)=Ent(D)-Ent(D|A)
$$

$$

$$

$$
Ent(D)=-\sum_{k=1}^K\frac{|C^k|}{|D|}\operatorname{log_2}\frac{|C^k|}{|D|}
$$


$$
\operatorname{Gain}(D,a)=\operatorname{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\operatorname{Ent}(D^v)
$$
$Ent(D)$是样本集$D$的熵，$Ent(D|A)$是数据集$D$对特征$A$的条件熵。$D_v$是$D$中特征$A$取第$i$个值得样本子集，$C_k$是D中属于第k类的样本子集。$V$是特征$A$取值的个数。$K$​是类的个数。

（2）样本集合D对特征A的信息增益比$(C4.5)$

信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响,著名的C4.5决策树算法不直接使用信息增益而是使用“增益率”(gain ratio)来选择最优划分属性。
$$
\operatorname{Gain}\text{ratio}(D,a)=\frac{\operatorname{Gain}(D,a)}{\operatorname{IV}(a)}
$$
其中
$$
\mathrm{IV}(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}
$$
称为属性$a$的固有值。

3）样本集合$D$的基尼指数$(CART)$

数据集D的纯度可以用基尼值来度量：
$$
\begin{aligned}\mathrm{Gini}(D)=&\sum_{k=1}^{|\mathcal{Y}|}\sum_{k^{\prime}\neq k}p_kp_{k^{\prime}}\\=&1-\sum_{k=1}^{|\mathcal{Y}|}p_k^2.\end{aligned}
$$
$Gimi(D)$越小，则数据集$D$的纯度越高.

属性a的基尼指数定义为
$$
\mathrm{Gini}\_\mathrm{index}(D,a)=\sum_{v=1}^{V}\frac{|D^v|}{|D|_.}\mathrm{Gini}(D^v)
$$
4．决策树的生成。通常使用信息增益最大、信息增益比最大或基尼指数最小作为特征选择的准则。决策树的生成往往通过计算信息增益或其他指标，从根结点开始，递归地产生决策树。这相当于用信息增益或其他准则不断地选取局部最优的特征，或将训练集分割为能够基本正确分类的子集。

5．决策树的剪枝。由于生成的决策树存在过拟合问题，需要对它进行剪枝，以简化学到的决策树。决策树的剪枝，往往从已生成的树上剪掉一些叶结点或叶结点以上的子树，并将其父结点或根结点作为新的叶结点，从而简化生成的决策树。
